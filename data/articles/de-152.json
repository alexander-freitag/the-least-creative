{
    "title": "Reasoning Fail: Gängige LLMs scheitern an kinderleichter Aufgabe",
    "timestamp": "2024-06-10T05:36:00Z",
    "content": "\n\n\n        Um logisches Denken steht es bei den aktuell gängigen KI-Modellen einfach nicht gut, das haben Forscher erneut bewiesen. \n      \n\n      \n        Buchstaben aus Holzklötzen zeigen die Worte F(AI)L.\n      \n    \n\n      (Bild: Shutterstock/FrankHH)\n    \nDie Aufgabe ist eigentlich ziemlich leicht: \"Alice hat N Brüder und M Schwestern. Wie viele Schwestern hat Alices Bruder?\" Während die meisten Erwachsenen – und laut der Autoren einer Studie auch Kinder – die Aufgabe wohl lösen können, scheitern die gängigen Großen Sprachmodelle, Large Language Models (LLM). Noch schlimmer, wie die Forscher finden, denn die KI-Modelle behaupten auch noch steif und fest, die richtige Antwort herausgefunden zu haben, wenn es die falsche war, und sie argumentieren logisch klingend, aber ebenfalls falsch. Ein bekanntes Problem der Sprachmodelle, das dennoch immer wieder überrascht – vor allem, da die Anbieter oft laut tönen, wie gut ihre Modelle bereits im sogenannten Reasoning seien – damit ist logisches Denken gemeint.\nGetestet wurden OpenAIs GPT 3.5, 4 und 4o, Claude 3 Opus von Anthropic, Googles Gemini sowie die offenen Modelle Llama 2 und 3 von Meta und Mistral und Mixtral von Mistral AI, Dbrx von Mosaic und Command R+ von Cohere. Die Antworten wurden statistisch erfasst und zeigen einen \"starken Zusammenbruch beim logischen Denken und eine Unfähigkeit, die einfache, oben formulierte Frage zu beantworten\". Ausnahmen machten GPT-4 und Claude 3, die zumindest manchmal richtig antworteten, heißt es in dem Paper, das von Forschenden des Juelich Supercomputing Center, dem Research Center Juelich, der School of Electrical und Electronic Engeneering der Universität Bristol und Laion veröffentlicht wurde. Laion ist eine Non-Profit-Organisation aus Deutschland, die Datensets und Modelle bereitstellt.\nNimmt man die weitläufig bekannte Metapher daher, LLMs seien stochastische Papageien, die also nur wiedergeben, was sie aufgeschnappt hätten, verwundert es nicht, dass sie an solchen Aufgaben scheitern. Der Vergleich mit dem Papageien stammt aus einem Paper von führenden KI-Forschern und Kritikern, zu denen unter anderem Emily M. Bender und Timnit Gebru gehören. Die Anbieter der gängigen KI-Modelle machen allerdings immer wieder große Versprechungen, wie gut ihre Modelle in Tests zum logischen Denken abschnitten.\nDagegen halten die Forschenden des Papers \"Alice in Wonderland\", wie sie das Problem mit dem Beantworten der Fragen zu Alice und ihren Geschwistern bezeichnen, die fehlenden Fähigkeiten für gefährlich: \"Dieser Zusammenbruch kann nicht nur deshalb als dramatisch angesehen werden, weil er bei einem so scheinbar Problem passiert, sondern auch, weil die Modelle dazu neigen, ihre falschen Lösungen als richtig zu bezeichnen, während sie oft Konfabulationen liefern, um die gegebene Antwort zusätzlich zu erklären, wobei sie einen argumentationsähnlichen Tonfall imitieren, aber unsinnige Argumente als Unterstützung für die ebenso unsinnigen, falschen, endgültigen Antworten liefern.\" Deshalb schlagen die Wissenschaftler auch vor, man müsse die bisherigen Benchmarks überdenken, da sie so simple Reasoning-Defizite nicht entdeckten.\nKeine Tech-News mehr verpassen: heise online auch bei WhatsApp abonnieren!\nWir schicken einmal am Tag die wichtigsten Nachrichten aus der Redaktion.\n\n\n(emw)\n\n\n\n    Keine News verpassen! Jeden Morgen der frische Nachrichtenüberblick von heise online\n  \n\n    Ausführliche Informationen zum Versandverfahren und zu Ihren\n    Widerrufsmöglichkeiten erhalten Sie in unserer\n    Datenschutzerklärung.\n  \n\nImmer informiert bleiben: Klicken Sie auf das Plus-Symbol an einem Thema, um diesem zu folgen. Wir zeigen Ihnen alle neuen Inhalte zu Ihren Themen.\n\n          Mehr erfahren.\n        \n\n\nExklusive Tests, Ratgeber & Hintergründe. Unbegrenzter Zugriff auf alle heise+ Beiträge inkl. allen Digital-Magazinen.",
    "translation": "Logical thinking is simply not good for the current AI models, as researchers have proven again. Letters from wooden blocks show the words F(AI)L. (Image: Shutterstock/FrankHH) The task is actually quite easy: \"Alice has N brothers and M sisters. How many sisters does Alice's brother have?\" While most adults – and according to the authors of a study also children – can solve the problem, the usual Large Language Models (LLM) fail. Even worse, as the researchers find, because the AI models also claim to have found out the right answer when it was the wrong one, and they argue logically, but also wrong. A well-known problem of language models, which nevertheless always surprises – especially since the vendors often sound loud, as their models are already meant in the so-called Reasoning – this is logical thinking. OpenAI's GPT 3.5, 4 and 4o, Claude 3 Opus from Anthropic, Google's Gemral 2 and the Mistral 3 from Mistral.The answers were recorded statistically and show a \"strong collapse in logical thinking and an inability to answer the simple question formulated above\". Exceptions were made by GPT-4 and Claude 3, which at least sometimes answered correctly, the paper published by researchers from the Juelich Supercomputing Center, the Research Center Juelich, the School of Electrical and Electronic Engeneering of the University of Bristol and Laion. Laion is a non-profit organization from Germany that provides data sets and models. Therefore, taking the widely known metaphor that LLMs are stochastic parrots, which only reflect what they would have picked up, it is not surprising that they fail in such tasks. The comparison with the parrot comes from a paper by leading AI researchers and critics, including Emily M. Bender and Timnit Gebru.The providers of the popular AI models, however, keep making big promises about how well their models cut off into logical thinking tests. On the other hand, the researchers in the \"Alice in Wonderland\" paper, as they call the problem of answering questions about Alice and her siblings, consider the lack of skills as dangerous: \"This collapse can not only be seen as dramatic because it happens in such a seemingly problem, but also because the models tend to call their false solutions correct, while they often provide confabulations to explain the given answer in addition, imitating an argument-like tone, but providing nonsensical arguments as support for the equally nonsensical, false, final answers.\" Therefore, the scientists also suggest that you have to rethink the previous benchmarks, as they do not discover such simple Reasoning Deficite. No more tech news is missing: heise online also at WhatsApp! We will send out the most important news from the editorial team once a day. (emw) No news to be missed! Tomorrow's fresh clicks on the background. Always informs about your online review.",
    "transformed_representation": [
        "Logic",
        "reasoning",
        "AI models",
        "language processing",
        "machine learning",
        "Large Language Models",
        "LLMs",
        "parrot metaphor",
        "stochastic processes",
        "Alice in Wonderland problem",
        "logical thinking deficit",
        "Reasoning Deficite."
    ],
    "detected_language": "de"
}