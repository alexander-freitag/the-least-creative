{"id": 158, "title": "Reasoning Fail: G\u00e4ngige LLMs scheitern an kinderleichter Aufgabe", "timestamp": "2024-06-10T05:36:00Z", "content": "\n\n\n        Um logisches Denken steht es bei den aktuell g\u00e4ngigen KI-Modellen einfach nicht gut, das haben Forscher erneut bewiesen. \n      \n\n      \n        Buchstaben aus Holzkl\u00f6tzen zeigen die Worte F(AI)L.\n      \n    \n\n      (Bild:\u00a0Shutterstock/FrankHH)\n    \nDie Aufgabe ist eigentlich ziemlich leicht: \"Alice hat N Br\u00fcder und M Schwestern. Wie viele Schwestern hat Alices Bruder?\" W\u00e4hrend die meisten Erwachsenen \u2013 und laut der Autoren einer Studie auch Kinder \u2013 die Aufgabe wohl l\u00f6sen k\u00f6nnen, scheitern die g\u00e4ngigen Gro\u00dfen Sprachmodelle, Large Language Models (LLM). Noch schlimmer, wie die Forscher finden, denn die KI-Modelle behaupten auch noch steif und fest, die richtige Antwort herausgefunden zu haben, wenn es die falsche war, und sie argumentieren logisch klingend, aber ebenfalls falsch. Ein bekanntes Problem der Sprachmodelle, das dennoch immer wieder \u00fcberrascht \u2013 vor allem, da die Anbieter oft laut t\u00f6nen, wie gut ihre Modelle bereits im sogenannten Reasoning seien \u2013 damit ist logisches Denken gemeint.\nGetestet wurden OpenAIs GPT 3.5, 4 und 4o, Claude 3 Opus von Anthropic, Googles Gemini sowie die offenen Modelle Llama 2 und 3 von Meta und Mistral und Mixtral von Mistral AI, Dbrx von Mosaic und Command R+ von Cohere. Die Antworten wurden statistisch erfasst und zeigen einen \"starken Zusammenbruch beim logischen Denken und eine Unf\u00e4higkeit, die einfache, oben formulierte Frage zu beantworten\". Ausnahmen machten GPT-4 und Claude 3, die zumindest manchmal richtig antworteten, hei\u00dft es in dem Paper, das von Forschenden des Juelich Supercomputing Center, dem Research Center Juelich, der School of Electrical und Electronic Engeneering der Universit\u00e4t Bristol und Laion ver\u00f6ffentlicht wurde. Laion ist eine Non-Profit-Organisation aus Deutschland, die Datensets und Modelle bereitstellt.\nNimmt man die weitl\u00e4ufig bekannte Metapher daher, LLMs seien stochastische Papageien, die also nur wiedergeben, was sie aufgeschnappt h\u00e4tten, verwundert es nicht, dass sie an solchen Aufgaben scheitern. Der Vergleich mit dem Papageien stammt aus einem Paper von f\u00fchrenden KI-Forschern und Kritikern, zu denen unter anderem Emily M. Bender und Timnit Gebru geh\u00f6ren. Die Anbieter der g\u00e4ngigen KI-Modelle machen allerdings immer wieder gro\u00dfe Versprechungen, wie gut ihre Modelle in Tests zum logischen Denken abschnitten.\nDagegen halten die Forschenden des Papers \"Alice in Wonderland\", wie sie das Problem mit dem Beantworten der Fragen zu Alice und ihren Geschwistern bezeichnen, die fehlenden F\u00e4higkeiten f\u00fcr gef\u00e4hrlich: \"Dieser Zusammenbruch kann nicht nur deshalb als dramatisch angesehen werden, weil er bei einem so scheinbar Problem passiert, sondern auch, weil die Modelle dazu neigen, ihre falschen L\u00f6sungen als richtig zu bezeichnen, w\u00e4hrend sie oft Konfabulationen liefern, um die gegebene Antwort zus\u00e4tzlich zu erkl\u00e4ren, wobei sie einen argumentations\u00e4hnlichen Tonfall imitieren, aber unsinnige Argumente als Unterst\u00fctzung f\u00fcr die ebenso unsinnigen, falschen, endg\u00fcltigen Antworten liefern.\" Deshalb schlagen die Wissenschaftler auch vor, man m\u00fcsse die bisherigen Benchmarks \u00fcberdenken, da sie so simple Reasoning-Defizite nicht entdeckten.\nKeine Tech-News mehr verpassen: heise online auch bei WhatsApp abonnieren!\nWir schicken einmal am Tag die wichtigsten Nachrichten aus der Redaktion.\n\n\n(emw)\n\n\n\n    Keine News verpassen! Jeden Morgen der frische Nachrichten\u00fcberblick von heise online\n  \n\n    Ausf\u00fchrliche Informationen zum Versandverfahren und zu Ihren\n    Widerrufsm\u00f6glichkeiten erhalten Sie in unserer\n    Datenschutzerkl\u00e4rung.\n  \n\nImmer informiert bleiben: Klicken Sie auf das Plus-Symbol an einem Thema, um diesem zu folgen. Wir zeigen Ihnen alle neuen Inhalte zu Ihren Themen.\n\n          Mehr erfahren.\n        \n\n\nExklusive Tests, Ratgeber & Hintergr\u00fcnde. Unbegrenzter Zugriff auf alle heise+ Beitr\u00e4ge inkl. allen Digital-Magazinen.", "keywords": ["Logic", "reasoning", "AI models", "language processing", "machine learning", "Large Language Models", "LLMs", "parrot metaphor", "stochastic processes", "Alice in Wonderland problem", "logical thinking deficit", "Reasoning Deficite."], "language": "de", "translation": "Logical thinking is simply not good for the current AI models, as researchers have proven again. Letters from wooden blocks show the words F(AI)L. (Image: Shutterstock/FrankHH) The task is actually quite easy: \"Alice has N brothers and M sisters. How many sisters does Alice's brother have?\" While most adults \u2013 and according to the authors of a study also children \u2013 can solve the problem, the usual Large Language Models (LLM) fail. Even worse, as the researchers find, because the AI models also claim to have found out the right answer when it was the wrong one, and they argue logically, but also wrong. A well-known problem of language models, which nevertheless always surprises \u2013 especially since the vendors often sound loud, as their models are already meant in the so-called Reasoning \u2013 this is logical thinking. OpenAI's GPT 3.5, 4 and 4o, Claude 3 Opus from Anthropic, Google's Gemral 2 and the Mistral 3 from Mistral.The answers were recorded statistically and show a \"strong collapse in logical thinking and an inability to answer the simple question formulated above\". Exceptions were made by GPT-4 and Claude 3, which at least sometimes answered correctly, the paper published by researchers from the Juelich Supercomputing Center, the Research Center Juelich, the School of Electrical and Electronic Engeneering of the University of Bristol and Laion. Laion is a non-profit organization from Germany that provides data sets and models. Therefore, taking the widely known metaphor that LLMs are stochastic parrots, which only reflect what they would have picked up, it is not surprising that they fail in such tasks. The comparison with the parrot comes from a paper by leading AI researchers and critics, including Emily M. Bender and Timnit Gebru.The providers of the popular AI models, however, keep making big promises about how well their models cut off into logical thinking tests. On the other hand, the researchers in the \"Alice in Wonderland\" paper, as they call the problem of answering questions about Alice and her siblings, consider the lack of skills as dangerous: \"This collapse can not only be seen as dramatic because it happens in such a seemingly problem, but also because the models tend to call their false solutions correct, while they often provide confabulations to explain the given answer in addition, imitating an argument-like tone, but providing nonsensical arguments as support for the equally nonsensical, false, final answers.\" Therefore, the scientists also suggest that you have to rethink the previous benchmarks, as they do not discover such simple Reasoning Deficite. No more tech news is missing: heise online also at WhatsApp! We will send out the most important news from the editorial team once a day. (emw) No news to be missed! Tomorrow's fresh clicks on the background. Always informs about your online review."}